---
title: "Untitled"
author: "Zhao Yuedian"
date: "2021/6/19"
output: html_document
---

```{r}
library(xgboost)
library(caret)
library(readxl)
library(dplyr)
library(tidyverse)
library(Metrics)
library(data.table)
```


```{r}
my_data_4 <- read_excel("my_data_dummies.xlsx")
```


```{r}
my_data_4$DyestuffUsage <- as.numeric(my_data_4$DyestuffUsage)
num_cols = c("Pre_Art_Actual_Time", "Dye_Art_Actual_Time", "Dye_Art_Sample_Time", "After_Art_Actual_Time", "After_Art_Sample_Time", "Fix_Art_Actual_Time", "Fix_Art_Sample_Time", "DyestuffUsage", "Dyeing_Ratio","Dye_weight","pure_dye","pure_after","pure_fix")
for (col in num_cols) set(my_data_4, which(is.na(my_data_4[[col]])), col, 0)
```

```{r}
set.seed(123)
indices <- sample(1:nrow(my_data_4), size = 0.95 * nrow(my_data_4))
train <- as.matrix(my_data_4[indices,])
test <- as.matrix(my_data_4[-indices,])
```

### S1

```{r}
s1_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 224],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s1_pred_xgb <- predict(s1_xgb, test[, 2:223])
s1_yhat <- s1_pred_xgb
s1_y <- test[, 224]
postResample(s1_yhat, s1_y)

s1_r <- s1_y - s1_yhat
hist(s1_r)
#  RMSE   Rsquared        MAE 
#31.9020873  0.8645089 17.3041760 
```

```{r}
#plot first 3 trees of model
#xgb.plot.tree(model = s1_xgb, trees = 0:2)

#importance_matrix <- xgb.importance(model = s1_xgb)
#xgb.plot.importance(importance_matrix, xlab = "Feature Importance")
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(3, 6, 1),
                          eta = seq(.2, .35, .01))
s1_xgb_train_rmse <- NULL
s1_xgb_test_rmse <- NULL

for (j in 1:64) {
  set.seed(123)
  s1_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 224],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s1_xgb_train_rmse[j] <- s1_xgb_untuned$evaluation_log$train_rmse_mean[s1_xgb_untuned$best_iteration]
  s1_xgb_test_rmse[j] <- s1_xgb_untuned$evaluation_log$test_rmse_mean[s1_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s1_xgb_test_rmse), ]
```

```{r}
s1_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 224],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 5,
    eta = .26
  )
```

```{r}
s1_pred_xgb_1 <- predict(s1_xgb_tuned, test[, 2:223])

s1_yhat_1 <- s1_pred_xgb_1
s1_y <- test[, 224]
postResample(s1_yhat_1, s1_y)

s1_r_1 <- s1_y - s1_yhat_1
hist(s1_r_1)
# RMSE   Rsquared        MAE 
#32.7715385  0.8574526 17.7707422 
```

### S2

```{r}
s2_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 225],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s2_pred_xgb <- predict(s2_xgb, test[, 2:223])
s2_yhat <- s2_pred_xgb
s2_y <- test[, 225]
postResample(s2_yhat, s2_y)

s2_r <- s2_y - s2_yhat
hist(s2_r)

# RMSE   Rsquared        MAE 
#22.0889518  0.9480261 10.3034909 
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(3, 6, 1),
                          eta = seq(.2, .35, .01))
s2_xgb_train_rmse <- NULL
s2_xgb_test_rmse <- NULL

for (j in 1:64) {
  set.seed(123)
  s2_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 225],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s2_xgb_train_rmse[j] <- s2_xgb_untuned$evaluation_log$train_rmse_mean[s2_xgb_untuned$best_iteration]
  s2_xgb_test_rmse[j] <- s2_xgb_untuned$evaluation_log$test_rmse_mean[s2_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s2_xgb_test_rmse), ]
```

```{r}
s2_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 225],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 3,
    eta = .34
  )
```

```{r}
s2_pred_xgb_1 <- predict(s2_xgb_tuned, test[, 2:223])

s2_yhat_1 <- s2_pred_xgb_1
s2_y <- test[, 225]
postResample(s2_yhat_1, s2_y)

s2_r_1 <- s2_y - s2_yhat_1
hist(s2_r_1)

#        RMSE   Rsquared        MAE 
#  20.9703086  0.9531763  9.9694447 
```


### S3

```{r}
s3_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 226],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s3_pred_xgb <- predict(s3_xgb, test[, 2:223])
s3_yhat <- s3_pred_xgb
s3_y <- test[, 226]
postResample(s3_yhat, s3_y)

s3_r <- s3_y - s3_yhat
hist(s3_r)

#  RMSE   Rsquared        MAE 
#52.7917993  0.2156947 31.2792356 
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(2, 6, 1),
                          eta = seq(.2, .5, .02))
s3_xgb_train_rmse <- NULL
s3_xgb_test_rmse <- NULL

for (j in 1:80) {
  set.seed(123)
  s3_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 226],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s3_xgb_train_rmse[j] <- s3_xgb_untuned$evaluation_log$train_rmse_mean[s3_xgb_untuned$best_iteration]
  s3_xgb_test_rmse[j] <- s3_xgb_untuned$evaluation_log$test_rmse_mean[s3_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s3_xgb_test_rmse), ]
```

```{r}
s3_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 226],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 3,
    eta = .24
  )
```

```{r}
s3_pred_xgb_1 <- predict(s3_xgb_tuned, test[, 2:223])

s3_yhat_1 <- s3_pred_xgb_1
s3_y <- test[, 226]
postResample(s3_yhat_1, s3_y)

s3_r_1 <- s3_y - s3_yhat_1
hist(s3_r_1)

#  RMSE   Rsquared        MAE 
#50.2929970  0.2517556 30.3931830  
```


### S4

```{r}
s4_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 227],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s4_pred_xgb <- predict(s4_xgb, test[, 2:223])
s4_yhat <- s4_pred_xgb
s4_y <- test[, 227]
postResample(s4_yhat, s4_y)

s4_r <- s4_y - s4_yhat
hist(s4_r)

# RMSE  Rsquared       MAE 
#35.675827  0.811067 19.232231 
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(2, 5, 1),
                          eta = seq(.2, .35, .01))
s4_xgb_train_rmse <- NULL
s4_xgb_test_rmse <- NULL

for (j in 1:64) {
  set.seed(123)
  s4_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 227],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s4_xgb_train_rmse[j] <- s4_xgb_untuned$evaluation_log$train_rmse_mean[s4_xgb_untuned$best_iteration]
  s4_xgb_test_rmse[j] <- s4_xgb_untuned$evaluation_log$test_rmse_mean[s4_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s4_xgb_test_rmse), ]
```

```{r}
s4_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 227],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 5,
    eta = .22
  )
```

```{r}
s4_pred_xgb_1 <- predict(s4_xgb_tuned, test[, 2:223])

s4_yhat_1 <- s4_pred_xgb_1
s4_y <- test[, 227]
postResample(s4_yhat_1, s4_y)

s4_r_1 <- s4_y - s4_yhat_1
hist(s4_r_1)

# RMSE   Rsquared        MAE 
#32.8016773  0.8325974 18.4864625 

```

### s5

```{r}
s5_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 228],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s5_pred_xgb <- predict(s5_xgb, test[, 2:223])
s5_yhat <- s5_pred_xgb
s5_y <- test[, 228]
postResample(s5_yhat, s5_y)

s5_r <- s5_y - s5_yhat
hist(s5_r)

#   RMSE   Rsquared        MAE 
#27.5435048  0.1474044 11.4252020 
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(2, 5, 1),
                          eta = seq(.2, .35, .01))
s5_xgb_train_rmse <- NULL
s5_xgb_test_rmse <- NULL

for (j in 1:64) {
  set.seed(123)
  s5_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 228],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s5_xgb_train_rmse[j] <- s5_xgb_untuned$evaluation_log$train_rmse_mean[s5_xgb_untuned$best_iteration]
  s5_xgb_test_rmse[j] <- s5_xgb_untuned$evaluation_log$test_rmse_mean[s5_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s5_xgb_test_rmse), ]
```

```{r}
s5_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 228],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 5,
    eta = .31
  )
```

```{r}
s5_pred_xgb_1 <- predict(s5_xgb_tuned, test[, 2:223])

s5_yhat_1 <- s5_pred_xgb_1
s5_y <- test[, 228]
postResample(s5_yhat_1, s5_y)

s5_r_1 <- s5_y - s5_yhat_1
hist(s5_r_1)

# RMSE   Rsquared        MAE 
# 27.3126545  0.1463923 11.6825439 

```

### s6

```{r}
s6_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 229],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s6_pred_xgb <- predict(s6_xgb, test[, 2:223])
s6_yhat <- s6_pred_xgb
s6_y <- test[, 229]
postResample(s6_yhat, s6_y)

s6_r <- s6_y - s6_yhat
hist(s6_r)

# RMSE   Rsquared        MAE 
# 30.9734569  0.5376355 16.3069684 
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(2, 5, 1),
                          eta = seq(.2, .35, .01))
s6_xgb_train_rmse <- NULL
s6_xgb_test_rmse <- NULL

for (j in 1:64) {
  set.seed(123)
  s6_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 229],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s6_xgb_train_rmse[j] <- s6_xgb_untuned$evaluation_log$train_rmse_mean[s6_xgb_untuned$best_iteration]
  s6_xgb_test_rmse[j] <- s6_xgb_untuned$evaluation_log$test_rmse_mean[s6_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s6_xgb_test_rmse), ]
```

```{r}
s6_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 229],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 3,
    eta = .27  )
```

```{r}
s6_pred_xgb_1 <- predict(s6_xgb_tuned, test[, 2:223])

s6_yhat_1 <- s6_pred_xgb_1
s6_y <- test[, 229]
postResample(s6_yhat_1, s6_y)

s6_r_1 <- s6_y - s6_yhat_1
hist(s6_r_1)

# RMSE   Rsquared        MAE 
# 31.6222741  0.5096121 17.0977407 

```
### s7

```{r}
s7_xgb <-
  xgboost(
    data = train[, 2:223],
    label = train[, 230],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )
```

```{r}
s7_pred_xgb <- predict(s7_xgb, test[, 2:223])
s7_yhat <- s7_pred_xgb
s7_y <- test[, 230]
postResample(s7_yhat, s7_y)

s7_r <- s7_y - s7_yhat
hist(s7_r)

#   RMSE   Rsquared        MAE 
#27.5435048  0.1474044 11.4252020 
```

```{r}
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(2, 5, 1),
                          eta = seq(.2, .35, .01))
s7_xgb_train_rmse <- NULL
s7_xgb_test_rmse <- NULL

for (j in 1:64) {
  set.seed(123)
  s7_xgb_untuned <- xgb.cv(
    data = train[, 2:223],
    label = train[, 230],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  s7_xgb_train_rmse[j] <- s7_xgb_untuned$evaluation_log$train_rmse_mean[s7_xgb_untuned$best_iteration]
  s7_xgb_test_rmse[j] <- s7_xgb_untuned$evaluation_log$test_rmse_mean[s7_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(s7_xgb_test_rmse), ]
```

```{r}
s7_xgb_tuned <-
  xgboost(
    data = train[, 2:223],
    label = train[, 230],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 2,
    eta = .23
  )
```

```{r}
s7_pred_xgb_1 <- predict(s7_xgb_tuned, test[, 2:223])

s7_yhat_1 <- s7_pred_xgb_1
s7_y <- test[, 230]
postResample(s7_yhat_1, s7_y)

s7_r_1 <- s7_y - s7_yhat_1
hist(s7_r_1)

# RMSE   Rsquared        MAE 
# 64.4737689  0.5658917 31.0570007 

```
```{r}
pred_xgb_total <- s1_pred_xgb_1 + s2_pred_xgb_1 + s3_pred_xgb_1 + s4_pred_xgb_1 + s5_pred_xgb_1 + s6_pred_xgb_1 + s7_pred_xgb_1
```

```{r}
residuals_xgb <- test[,231] - pred_xgb_total
quantile(residuals_xgb,  probs = c(10,25,75,90)/100)
#write.csv(pred_xgb_total,'xgbtotal_output.csv')
```

```{r}
#rmse
sqrt(sum(residuals_xgb^2)/nrow(residuals_xgb))
```


```{r}
new_res_xgb <- residuals_xgb - mean(residuals_xgb)
print("90mins:")
length(new_res_xgb[new_res_xgb<90&new_res_xgb>-90])/length(new_res_xgb)
print("30mins:")
length(new_res_xgb[new_res_xgb<30&new_res_xgb>-30])/length(new_res_xgb)
```







